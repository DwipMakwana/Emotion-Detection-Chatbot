{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcV6LXzlRlwk"
      },
      "source": [
        "### Setting Up Your Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "yVct1tUNRUdq",
        "outputId": "9afbdc9b-e29c-46c9-988c-f0ee0aa89d59"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "#!pip install tensorflow opencv-python matplotlib pandas scikit-learn transformers\n",
        "#!pip install face-recognition dlib\n",
        "#!pip install gradio # For creating a simple demo interface"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rnFpRIGRrQ3"
      },
      "source": [
        "### Data Collection & Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "7utkb3CuRY9Z",
        "outputId": "810a1930-52da-4324-d6b1-b9babe53b24a"
      },
      "outputs": [],
      "source": [
        "# Download and extract the FER2013 dataset\n",
        "#!wget https://www.kaggle.com/datasets/msambare/fer2013/download -O fer2013.zip\n",
        "#!unzip fer2013.zip -d data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGrrMOsxRxHA"
      },
      "source": [
        "### Model Building"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkBJSYu8RgWC",
        "outputId": "a9d25a5c-e70f-416b-8486-4e7c00d8b641"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\dwipm\\.conda\\envs\\ml-venv-39\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Define the CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48, 48, 1)),\n",
        "    Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Dropout(0.25),\n",
        "\n",
        "    Conv2D(128, kernel_size=(3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(128, kernel_size=(3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Dropout(0.25),\n",
        "\n",
        "    Flatten(),\n",
        "    Dense(1024, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(7, activation='softmax')  # 7 emotions: angry, disgust, fear, happy, sad, surprise, neutral\n",
        "])\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.0001), metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvkmampfUB_X",
        "outputId": "2105cdd4-d0ac-428a-ecd4-20ed64af51cb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\dwipm\\AppData\\Local\\Temp\\ipykernel_13904\\1573050073.py:8: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "  base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(48, 48, 3))\n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model created and saved!\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def create_and_save_model():\n",
        "    # Load base model\n",
        "    base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(48, 48, 3))\n",
        "\n",
        "    # Add custom layers\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(512, activation='relu')(x)\n",
        "    predictions = Dense(7, activation='softmax')(x)\n",
        "\n",
        "    # Create model\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Save model\n",
        "    model.save('emotion_model.h5')\n",
        "    print(\"Model created and saved!\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# Create and save model\n",
        "model = create_and_save_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7GLElsMR0A7"
      },
      "source": [
        "### Fine-tuning with Transfer Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb-vX3WsRg8M",
        "outputId": "188f2f7a-f175-4a6a-ad30-26c90d03e322"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\dwipm\\AppData\\Local\\Temp\\ipykernel_13904\\3093589820.py:6: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "  base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(48, 48, 3))\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Load pre-trained MobileNetV2\n",
        "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(48, 48, 3))\n",
        "\n",
        "# Add custom layers for emotion detection\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(512, activation='relu')(x)\n",
        "predictions = Dense(7, activation='softmax')(x)\n",
        "\n",
        "# Create and compile the model\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Freeze base model layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gD8xqJWNR2bg"
      },
      "source": [
        "### Real-time Facial Emotion Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4-ibSInR3wL",
        "outputId": "69bf3f26-1cf3-49f8-afbb-6f04a5cd0045"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "\n",
        "# Load the pre-trained model\n",
        "model = load_model('emotion_model.h5')\n",
        "\n",
        "# Define emotion labels\n",
        "emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
        "\n",
        "# Load haar cascade for face detection\n",
        "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "def detect_emotion(frame):\n",
        "    # Convert to grayscale\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Detect faces\n",
        "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
        "\n",
        "    for (x, y, w, h) in faces:\n",
        "        # Extract face ROI\n",
        "        roi_gray = gray[y:y+h, x:x+w]\n",
        "        roi_gray = cv2.resize(roi_gray, (48, 48))\n",
        "        roi_gray = roi_gray.astype('float') / 255.0\n",
        "        roi_gray = img_to_array(roi_gray)\n",
        "        roi_gray = np.expand_dims(roi_gray, axis=0)\n",
        "\n",
        "        # Make prediction\n",
        "        prediction = model.predict(roi_gray)[0]\n",
        "        emotion_idx = np.argmax(prediction)\n",
        "        emotion = emotion_labels[emotion_idx]\n",
        "        confidence = prediction[emotion_idx] * 100\n",
        "\n",
        "        # Draw bounding box and emotion label\n",
        "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
        "        label = f\"{emotion}: {confidence:.2f}%\"\n",
        "        cv2.putText(frame, label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
        "\n",
        "    return frame, emotion if len(faces) > 0 else 'No face detected'\n",
        "\n",
        "# Function to process webcam feed (for testing locally)\n",
        "def process_webcam():\n",
        "    cap = cv2.VideoCapture(0)\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame, detected_emotion = detect_emotion(frame)\n",
        "\n",
        "        cv2.imshow('Emotion Detection', frame)\n",
        "\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "            break\n",
        "\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v48r5bvTR9DA"
      },
      "source": [
        "### Chatbot Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "H8YKJ13jVKz1",
        "outputId": "971744f4-3a89-4369-e3b7-54bc6314670c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on local URL:  http://127.0.0.1:7861\n",
            "IMPORTANT: You are using gradio version 3.50.2, however version 4.44.1 is available, please upgrade.\n",
            "--------\n",
            "Running on public URL: https://c1b8420af6ca1f7a3a.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://c1b8420af6ca1f7a3a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gradio as gr\n",
        "import cv2\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Define emotion labels\n",
        "emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
        "\n",
        "# Simple function to detect faces and assign random emotions\n",
        "def detect_emotion(image):\n",
        "    if image is None:\n",
        "        return image, \"No image provided\"\n",
        "    \n",
        "    # Convert to grayscale for face detection\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "    \n",
        "    # Load face detector\n",
        "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
        "    \n",
        "    result_image = image.copy()\n",
        "    \n",
        "    if len(faces) > 0:\n",
        "        for (x, y, w, h) in faces:\n",
        "            # For demo: random emotion\n",
        "            emotion = random.choice(emotion_labels)\n",
        "            \n",
        "            # Draw rectangle around face\n",
        "            cv2.rectangle(result_image, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
        "            \n",
        "            # Draw emotion label\n",
        "            cv2.putText(result_image, emotion, (x, y-10), \n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (36, 255, 12), 2)\n",
        "    else:\n",
        "        emotion = \"No face detected\"\n",
        "    \n",
        "    return result_image, emotion\n",
        "\n",
        "# Simple function to generate chatbot responses\n",
        "def get_response(emotion, message):\n",
        "    emotion_responses = {\n",
        "        'Happy': \"You seem happy! That's great!\",\n",
        "        'Sad': \"I notice you might be feeling down. How can I help?\",\n",
        "        'Angry': \"I sense you might be frustrated. Would you like to talk about it?\",\n",
        "        'Surprise': \"You look surprised! What's going on?\",\n",
        "        'Neutral': \"How can I assist you today?\",\n",
        "        'Fear': \"You seem concerned. Is everything okay?\",\n",
        "        'Disgust': \"Something seems to be bothering you. Can I help?\",\n",
        "        'No face detected': \"I can't see your face clearly. Can you try another image?\"\n",
        "    }\n",
        "    \n",
        "    response = emotion_responses.get(emotion, \"How can I help you today?\")\n",
        "    \n",
        "    if message:\n",
        "        if \"hello\" in message.lower() or \"hi\" in message.lower():\n",
        "            response += \" Hello there! What can I do for you?\"\n",
        "        elif \"how are you\" in message.lower():\n",
        "            response += \" I'm doing well, thanks for asking!\"\n",
        "        elif \"bye\" in message.lower():\n",
        "            response += \" Goodbye! Have a great day!\"\n",
        "        else:\n",
        "            response += \" Tell me more about how I can help you.\"\n",
        "    \n",
        "    return response\n",
        "\n",
        "# Chatbot interface function\n",
        "def chatbot_interface(image, message):\n",
        "    # Process image to detect emotion\n",
        "    processed_image, emotion = detect_emotion(image)\n",
        "    \n",
        "    # Generate response based on emotion and message\n",
        "    response = get_response(emotion, message)\n",
        "    \n",
        "    return processed_image, response, emotion\n",
        "\n",
        "# Create a simple Gradio interface\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# Simple Emotion Chatbot\")\n",
        "    \n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            camera_input = gr.Image(source=\"webcam\", label=\"Camera Input\")\n",
        "            text_input = gr.Textbox(label=\"Your Message\")\n",
        "            submit_btn = gr.Button(\"Send\")\n",
        "        \n",
        "        with gr.Column():\n",
        "            image_output = gr.Image(label=\"Emotion Detection\")\n",
        "            text_output = gr.Textbox(label=\"Chatbot Response\")\n",
        "    \n",
        "    submit_btn.click(\n",
        "        fn=chatbot_interface,\n",
        "        inputs=[camera_input, text_input],\n",
        "        outputs=[image_output, text_output]\n",
        "    )\n",
        "\n",
        "# Launch the app\n",
        "demo.launch(share=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ml-venv-39",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
